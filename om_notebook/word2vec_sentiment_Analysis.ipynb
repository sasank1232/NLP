{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2590e3b1-90e5-4608-ba5d-0a9e49442a47",
   "metadata": {},
   "source": [
    "## Step-1\n",
    "\n",
    "**Import the packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce8a7463-98d7-4da8-bf4a-425b06c974d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omkar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\omkar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24687b56-d3dc-4ccb-acd9-9ef8eb1b6a5a",
   "metadata": {},
   "source": [
    "## Step-2\n",
    "**Read the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82b73cfc-8d33-4e6b-9b71-16ea754eb35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('Sentiment_analaysis_word_2_vec_finetune\\IMDB Dataset_sample.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0077f5-21d3-4e1a-b180-96547518db97",
   "metadata": {},
   "source": [
    "## Step-3\n",
    "\n",
    "**Preprocess the text data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebdcb165-6016-4265-a276-c98fce0d882c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[one, reviewers, mentioned, watching, oz, epis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[wonderful, little, production, br, br, filmin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[basically, theres, family, little, boy, jake,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[petter, matteis, love, time, money, visually,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>I have not read the book that this was based u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[read, book, based, uponinspired, ofthe, other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>Brilliant thriller, deserving far more fame, M...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[brilliant, thriller, deserving, far, fame, mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>I saw this movie a fews years ago and was lite...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[saw, movie, fews, years, ago, literally, swep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>I just watched this movie on Bravo! and it was...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[watched, movie, bravo, absolutely, horrible, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>How nice to have a movie the entire family can...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[nice, movie, entire, family, watch, together,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1114 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review sentiment  \\\n",
       "0     One of the other reviewers has mentioned that ...  positive   \n",
       "1     A wonderful little production. <br /><br />The...  positive   \n",
       "2     I thought this was a wonderful way to spend ti...  positive   \n",
       "3     Basically there's a family where a little boy ...  negative   \n",
       "4     Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "...                                                 ...       ...   \n",
       "1109  I have not read the book that this was based u...  positive   \n",
       "1110  Brilliant thriller, deserving far more fame, M...  positive   \n",
       "1111  I saw this movie a fews years ago and was lite...  positive   \n",
       "1112  I just watched this movie on Bravo! and it was...  negative   \n",
       "1113  How nice to have a movie the entire family can...  positive   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [one, reviewers, mentioned, watching, oz, epis...  \n",
       "1     [wonderful, little, production, br, br, filmin...  \n",
       "2     [thought, wonderful, way, spend, time, hot, su...  \n",
       "3     [basically, theres, family, little, boy, jake,...  \n",
       "4     [petter, matteis, love, time, money, visually,...  \n",
       "...                                                 ...  \n",
       "1109  [read, book, based, uponinspired, ofthe, other...  \n",
       "1110  [brilliant, thriller, deserving, far, fame, mi...  \n",
       "1111  [saw, movie, fews, years, ago, literally, swep...  \n",
       "1112  [watched, movie, bravo, absolutely, horrible, ...  \n",
       "1113  [nice, movie, entire, family, watch, together,...  \n",
       "\n",
       "[1114 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "# Apply preprocessing to the text data\n",
    "df['tokens'] = df['review'].apply(preprocess_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ee4ca-15f4-418f-9c06-d27f01dd1530",
   "metadata": {},
   "source": [
    "$Step-4$\n",
    "\n",
    "**Train the Word2Vec Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f824e2c3-85e5-47d5-ad34-7a2f21b578e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b71fb70f-d9e2-4e56-9792-a96d88dde9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x229c8771650>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77188a39-e60f-4253-8885-19c0ef1c9dcd",
   "metadata": {},
   "source": [
    "**model vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab1799e-c887-4a0e-8647-88d2349ce68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21781\n"
     ]
    }
   ],
   "source": [
    "# Get the list of words in the vocabulary\n",
    "vocabulary_list = list(model.wv.index_to_key)\n",
    "\n",
    "# Print the vocabulary\n",
    "print(len(vocabulary_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef52959c-7152-41c0-9c81-1c88f6a0049f",
   "metadata": {},
   "source": [
    "**word vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7a6560d-bef5-4626-b079-9fbb4299a269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'reviewers',\n",
       " 'mentioned',\n",
       " 'watching',\n",
       " 'oz',\n",
       " 'episode',\n",
       " 'youll',\n",
       " 'hooked',\n",
       " 'right',\n",
       " 'exactly',\n",
       " 'happened',\n",
       " 'mebr',\n",
       " 'br',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'struck',\n",
       " 'oz',\n",
       " 'brutality',\n",
       " 'unflinching',\n",
       " 'scenes',\n",
       " 'violence',\n",
       " 'set',\n",
       " 'right',\n",
       " 'word',\n",
       " 'go',\n",
       " 'trust',\n",
       " 'show',\n",
       " 'faint',\n",
       " 'hearted',\n",
       " 'timid',\n",
       " 'show',\n",
       " 'pulls',\n",
       " 'punches',\n",
       " 'regards',\n",
       " 'drugs',\n",
       " 'sex',\n",
       " 'violence',\n",
       " 'hardcore',\n",
       " 'classic',\n",
       " 'use',\n",
       " 'wordbr',\n",
       " 'br',\n",
       " 'called',\n",
       " 'oz',\n",
       " 'nickname',\n",
       " 'given',\n",
       " 'oswald',\n",
       " 'maximum',\n",
       " 'security',\n",
       " 'state',\n",
       " 'penitentary',\n",
       " 'focuses',\n",
       " 'mainly',\n",
       " 'emerald',\n",
       " 'city',\n",
       " 'experimental',\n",
       " 'section',\n",
       " 'prison',\n",
       " 'cells',\n",
       " 'glass',\n",
       " 'fronts',\n",
       " 'face',\n",
       " 'inwards',\n",
       " 'privacy',\n",
       " 'high',\n",
       " 'agenda',\n",
       " 'em',\n",
       " 'city',\n",
       " 'home',\n",
       " 'manyaryans',\n",
       " 'muslims',\n",
       " 'gangstas',\n",
       " 'latinos',\n",
       " 'christians',\n",
       " 'italians',\n",
       " 'irish',\n",
       " 'moreso',\n",
       " 'scuffles',\n",
       " 'death',\n",
       " 'stares',\n",
       " 'dodgy',\n",
       " 'dealings',\n",
       " 'shady',\n",
       " 'agreements',\n",
       " 'never',\n",
       " 'far',\n",
       " 'awaybr',\n",
       " 'br',\n",
       " 'would',\n",
       " 'say',\n",
       " 'main',\n",
       " 'appeal',\n",
       " 'show',\n",
       " 'due',\n",
       " 'fact',\n",
       " 'goes',\n",
       " 'shows',\n",
       " 'wouldnt',\n",
       " 'dare',\n",
       " 'forget',\n",
       " 'pretty',\n",
       " 'pictures',\n",
       " 'painted',\n",
       " 'mainstream',\n",
       " 'audiences',\n",
       " 'forget',\n",
       " 'charm',\n",
       " 'forget',\n",
       " 'romanceoz',\n",
       " 'doesnt',\n",
       " 'mess',\n",
       " 'around',\n",
       " 'first',\n",
       " 'episode',\n",
       " 'ever',\n",
       " 'saw',\n",
       " 'struck',\n",
       " 'nasty',\n",
       " 'surreal',\n",
       " 'couldnt',\n",
       " 'say',\n",
       " 'ready',\n",
       " 'watched',\n",
       " 'developed',\n",
       " 'taste',\n",
       " 'oz',\n",
       " 'got',\n",
       " 'accustomed',\n",
       " 'high',\n",
       " 'levels',\n",
       " 'graphic',\n",
       " 'violence',\n",
       " 'violence',\n",
       " 'injustice',\n",
       " 'crooked',\n",
       " 'guards',\n",
       " 'wholl',\n",
       " 'sold',\n",
       " 'nickel',\n",
       " 'inmates',\n",
       " 'wholl',\n",
       " 'kill',\n",
       " 'order',\n",
       " 'get',\n",
       " 'away',\n",
       " 'well',\n",
       " 'mannered',\n",
       " 'middle',\n",
       " 'class',\n",
       " 'inmates',\n",
       " 'turned',\n",
       " 'prison',\n",
       " 'bitches',\n",
       " 'due',\n",
       " 'lack',\n",
       " 'street',\n",
       " 'skills',\n",
       " 'prison',\n",
       " 'experience',\n",
       " 'watching',\n",
       " 'oz',\n",
       " 'may',\n",
       " 'become',\n",
       " 'comfortable',\n",
       " 'uncomfortable',\n",
       " 'viewingthats',\n",
       " 'get',\n",
       " 'touch',\n",
       " 'darker',\n",
       " 'side']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "134c1328-2d76-4b20-9344-9dcd951e3495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv['one'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e3791-adaa-4187-8abb-841a3e3487ec",
   "metadata": {},
   "source": [
    "## Step-5\n",
    "\n",
    "**Create Sentence Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d4d4848-b323-4eac-88c2-3d2e38f51976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(tokens, model):\n",
    "    word_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(100)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Create sentence vectors\n",
    "df['sentence_vector'] = df['tokens'].apply(lambda tokens: get_sentence_vector(tokens, model))\n",
    "X = np.vstack(df['sentence_vector'])\n",
    "y = df['sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca8b13c6-49ce-4f6d-8646-6484842e98c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0960805 ,  0.22906654,  0.07162032, ..., -0.21624544,\n",
       "         0.03138896,  0.067889  ],\n",
       "       [-0.0995374 ,  0.24531554,  0.06818158, ..., -0.22688092,\n",
       "         0.03846664,  0.08647707],\n",
       "       [-0.10225654,  0.25646853,  0.06438303, ..., -0.237278  ,\n",
       "         0.02870193,  0.07385214],\n",
       "       ...,\n",
       "       [-0.10755026,  0.24893445,  0.07027991, ..., -0.23195483,\n",
       "         0.02891569,  0.07173766],\n",
       "       [-0.10833995,  0.27410564,  0.04659814, ..., -0.26424256,\n",
       "         0.02913951,  0.07571834],\n",
       "       [-0.10312305,  0.25518674,  0.06371851, ..., -0.23492046,\n",
       "         0.02639818,  0.06758828]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6859867-a43b-4473-a5f1-121821a02a54",
   "metadata": {},
   "source": [
    "## Step-6\n",
    "\n",
    "**Train a Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6791ff04-d0ed-4449-bf34-523add22c35f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9673aa4d-5d8e-42ee-8526-96cd902d0a26",
   "metadata": {},
   "source": [
    "## Step-7\n",
    "**Evaluate the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d72084df-8c43-4357-acc2-5536e84f914a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 61.43%\n"
     ]
    }
   ],
   "source": [
    "# Predict sentiment\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b7d16ff-dc40-4bb1-abf3-0901ee986db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 61.43%\n",
      "Gradient Boosting Accuracy: 68.16%\n",
      "AdaBoost Accuracy: 69.51%\n",
      "Support Vector Classifier Accuracy: 57.40%\n",
      "Logistic Regression Accuracy: 60.99%\n",
      "Neural Network Accuracy: 66.37%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    'Support Vector Classifier': SVC(kernel='linear', random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "\n",
    "# Print the results\n",
    "for name, accuracy in results.items():\n",
    "    print(f\"{name} Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1894f3ad-56cf-4559-8006-5003a5fcbed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ada_boost_model.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifier=AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "best_classifier\n",
    "joblib.dump(classifier, 'ada_boost_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81438803-f091-44fc-a716-3ca3fed9627d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mlp_model.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifier=MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000)\n",
    "joblib.dump(classifier, 'mlp_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9db4a6-8a25-455d-a5f1-b5802b007205",
   "metadata": {},
   "source": [
    "# Step-8\n",
    "**Save the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f84e711-7487-49c9-b954-05912f64501e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x229c8771650>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1405b92-f910-444e-97e5-ef5ac5df2672",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Word_2_Vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "181d1f1d-d24b-45a7-9fa3-529780dc807a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random_forest_model.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(classifier, 'random_forest_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f046aa-ee99-493f-95f9-73d21ab45be6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
